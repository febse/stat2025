{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability\n",
    "\n",
    "Open in Google Colab: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/febse/stat2024/blob/main/05-Conditional-Probability.ipynb)\n",
    "\n",
    "\n",
    "Until now we have seen some basic properties of probability. In this part of the course we will learn how to account\n",
    "for partial information when calculating probabilities.\n",
    "\n",
    "Suppose that you are playing a game of two dice and you want to bet that the first die will show a 6. A fried of yours rolls the dice and tells you that the sum of the two dice is 7. What is the probability that the first die shows a 6 _given_ that the sum of the two dice is 7? Would this additional information help you when deciding whether to bet on the first die showing a 6?\n",
    "\n",
    "This is an example of a _conditional probability_ problem.\n",
    "\n",
    ":::{#def-conditional-probability}\n",
    "\n",
    "Let $A$ and $B$ be two events. The conditional probability of $A$ given $B$ is defined as\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "We read $P(A | B)$ as \"the probability of $A$ given $B$\".\n",
    ":::\n",
    "\n",
    "Similar to the definition of a probability law that we have seen before, with equally likely outcomes in the sample \n",
    "space, we count the number of outcomes in the event $A$ that are also in the event $B$ and divide by the number of outcomes in $B$.\n",
    "\n",
    "In the dice example we have $6^2=36$ equally likely outcomes in the sample space. \n",
    "\n",
    "$$\n",
    "\\Omega = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (1,1), (1,2), (1, 3), (1, 4), (1, 5), \\mathbf{(1, 6)} \\\\\n",
    "    (2, 1), (2, 2), (2, 3), (2, 4), \\mathbf{(2, 5)}, (2, 6) \\\\\n",
    "    (3, 1), (3, 2), (3, 3), \\mathbf{(3, 4)}, (3, 5), (3, 6) \\\\\n",
    "    (4, 1), (4, 2), \\mathbf{(4, 3)}, (4, 4), (4, 5), (4, 6) \\\\\n",
    "    (5, 1), \\mathbf{(5, 2)}, (5, 3), (5, 4), (5, 5), (5, 6) \\\\\n",
    "    \\mathbf{(6, 1)}, (6, 2), (6, 3), (6, 4), (6, 5), (6, 6) \\\\\n",
    "    \\end{align*}    \n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "The event $B$ (sum equal to 7) consists of the following elements:\n",
    "\n",
    "$$\n",
    "B = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1) \\\\\n",
    "    \\end{align*}\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "The event $A$ (first die shows a 6) consists of the following elements:\n",
    "\n",
    "$$\n",
    "A = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6) \\\\\n",
    "    \\end{align*}\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "Now you know that one of the outcomes in B has occurred (otherwise the sum will not be equal to 7). There are 6 outcomes in $B$ and one of them is also in $A$, so the \n",
    "intersection of $A$ and $B$ is:\n",
    "\n",
    "$$\n",
    "A \\cap B = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (6, 1) \\\\\n",
    "    \\end{align*}\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "Therefore, the conditional probability of $A$ given $B$ is $1/6$.\n",
    "\n",
    "What about the _unconditional_ probability of $A$?\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{6}{36} = \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "In this case the conditional probability of $A$ given $B$ is the same as the unconditional probability of $A$, meaning that knowing that the sum was 7 does not help you in deciding whether to bet on the first die showing a 6 (it does not change the odds). We say that the two events are **indepedent** (under $P$).\n",
    "\n",
    "Consider, however, that the friend tells you that the sum of the two dice is 10. What is the probability that the first die shows a 6 _given_ that the sum of the two dice is 10? Let's denote this event as $C$.\n",
    "\n",
    "In this case the event $C$ consists of the following elements:\n",
    "\n",
    "$$\n",
    "C = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (4, 6), (5, 5), (6, 4) \\\\\n",
    "    \\end{align*}\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "The elements in $A$ are the same, but now we are looking for the elements which are both in $A$ and $C$. \n",
    "There is only one element that is in both $A$ and $C$:\n",
    "\n",
    "$$\n",
    "A \\cap C = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (6, 4) \\\\\n",
    "    \\end{align*}\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "So the _conditional_ probability of $A$ given $C$ is $1/3$.\n",
    "$$\n",
    "P(A|C) = \\frac{\\text{number of elements in } A \\cap C}{\\text{number of elements in } C} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "So this time the knowledge that the sum is 10 is informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence of Two Events\n",
    "\n",
    ":::{#def-independence-AB}\n",
    "## Independence of Two Events\n",
    "\n",
    "Two events $A$ and $B$ are said to be independent if\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "The definition in @def-independence-AB is equivalent to the following, but has the advantage of being symmetric in $A$ and $B$:\n",
    "\n",
    ":::{#exr-independence-AB}\n",
    "## Independence and Conditional Probability\n",
    "\n",
    "Show that the definition of independence in @def-independence-AB is equivalent to the following:\n",
    "\n",
    "1. $P(A|B) = P(A)$\n",
    "2. $P(B|A) = P(B)$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{#exr-independence-AB-implication}\n",
    "## Implications of Independence\n",
    "\n",
    "Show that if $A$ and $B$ are independent, then the following statements are true:\n",
    "\n",
    "1. $A$ and $B^c$ are independent\n",
    "2. $A^c$ and $B$ are independent\n",
    "3. $A^c$ and $B^c$ are independent\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "## Solution (click to expand)\n",
    "\n",
    "From the definition of independence we have\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "1. $A$ and $B^c$ are independent\n",
    "\n",
    "If $A$ and $B$ are independent, then it must be the case that\n",
    "\n",
    "$$\n",
    "P(A \\cap B^c) = P(A) \\cdot P(B^c)\n",
    "$$\n",
    "\n",
    "To show this, we can use the fact that $B$ and $B^c$ are disjoint and that $B \\cup B^c = \\Omega$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A) = P(A \\cap \\Omega) &= P(A \\cap (B \\cup B^c)) \\\\\n",
    "&= P((A \\cap B) \\cup (A \\cap B^c)) \\\\\n",
    "&= P(A \\cap B) + P(A \\cap B^c) \\\\\n",
    "&= P(A) \\cdot P(B) + P(A \\cap B^c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can now solve for $P(A \\cap B^c)$\n",
    "\n",
    "$$\n",
    "P(A \\cap B^c) = P(A) - P(A) \\cdot P(B) = P(A) \\cdot (1 - P(B)) = P(A) \\cdot P(B^c)\n",
    "$$\n",
    "\n",
    "So we showed the condition of independence for $A$ and $B^c$.\n",
    "\n",
    "$$\n",
    "P(A \\cap B^c) = P(A) \\cdot P(B^c)\n",
    "$$\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    ":::{#exr-independence-AB-disjoint}\n",
    "## Disjoint Events and Independence\n",
    "\n",
    "Let $A$ and $B$ be two events such that $P(A) > 0$ and $P(B) > 0$. Show that if $A$ and $B$ are disjoint, then they are not independent.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Independence\n",
    "\n",
    "The definition of independence can be extended to more than two events. However, an attempt to extend the definition \n",
    "by saying that $A$, $B$, and $C$ are independent if $P(A \\cap B \\cap C) = P(A) \\cdot P(B) \\cdot P(C)$ does not ensure that $A$, $B$, and $C$ are _pairwise_ independent in the sense of @def-independence-AB. If the events are pairwise independent, there is no guarantee that the probability of their intersection\n",
    "is the product of their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":::{#def-mutual-independence}\n",
    "## Mutual Independence\n",
    "\n",
    "Events $A_1, A_2, \\ldots, A_n$ are said to be mutually independent if for every subset $I \\subseteq \\{1, 2, \\ldots, n\\}$ we have\n",
    "\n",
    "$$\n",
    "P\\left(\\bigcap_{i \\in I} A_i\\right) = \\prod_{i \\in I} P(A_i)\n",
    "$$\n",
    "\n",
    "For example for $n=3$ we have that $A_1$, $A_2$, and $A_3$ are mutually independent if _all_ of the following conditions hold:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A_1 \\cap A_2) &= P(A_1) \\cdot P(A_2) \\\\\n",
    "P(A_1 \\cap A_3) &= P(A_1) \\cdot P(A_3) \\\\\n",
    "P(A_2 \\cap A_3) &= P(A_2) \\cdot P(A_3) \\\\\n",
    "P(A_1 \\cap A_2 \\cap A_3) &= P(A_1) \\cdot P(A_2) \\cdot P(A_3) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":::{#exr-mutual-independence-contradicition}\n",
    "## Mutual Independence and Pairwise Independence\n",
    "\n",
    "Consider a sample space with 9 equally likely outcomes, each consisting of a triple $(i, j, k)$ where $i, j, k \\in \\{1, 2, 3\\}$.\n",
    "\n",
    "$$\n",
    "\\Omega = \\left\\{\n",
    "    \\begin{align*}\n",
    "    (1, 1, 1), (2, 2, 2), (3, 3, 3) \\\\\n",
    "    (1, 2, 3), (1, 3, 2), (3, 2, 1) \\\\\n",
    "    (3, 1, 2), (2, 1, 3), (2, 3, 1) \\\\\n",
    "    \\end{align*}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Consider the following events $A_i = \\{\\text{i-th place of the outcome is 1}\\}$. Are the events $A_1$, $A_2$, and $A_3$ pairwise independent? Are they mutually independent?\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "We have already seen that the conditional probability of $A$ given $B$ is defined as\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "We can also write this as\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A|B) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "and similarly\n",
    "\n",
    "$$\n",
    "P(B \\cap A) = P(B|A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Because the first two terms are equal, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A) \\\\\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This last equation is known as **Bayes' Theorem** and it turns out that it holds in a more general way than here.\n",
    "\n",
    ":::{#thm-bayes-theorem}\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Let $A$ and $B$ be two events such that $P(A) > 0$ and $P(B) > 0$. Then\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Furthermore, let $A_1, A_2, \\ldots, A_n$ be a partition of the sample space $\\Omega$. A partition is a collection of disjoint events such that their union is the entire sample space:\n",
    "\n",
    "$$\n",
    "A_i \\cap A_j = \\emptyset \\quad \\text{for all } i \\neq j \\quad \\text{and} \\quad \\bigcup_{i=1}^n A_i = \\Omega\n",
    "$$\n",
    "\n",
    "For any event $B$ such that $P(B) > 0$, we have\n",
    "\n",
    "$$\n",
    "P(A_i|B) = \\frac{P(B|A_i) \\cdot P(A_i)}{\\sum_{j=1}^n P(B|A_j) \\cdot P(A_j)}\n",
    "$$\n",
    "\n",
    "To understand the derivation of Bayes' Theorem, see that it holds for a very simple partition. The collection of events $A$ and $A^c$ is a partition of the sample space. In this case, Bayes' Theorem becomes\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\n",
    "$$\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-total-probability-complements}\n",
    "## Total Probability and Complements\n",
    "\n",
    "Let $A$ and $B$ be some events such that $P(A) > 0$ and $P(B) > 0$. Show that\n",
    "\n",
    "$$\n",
    "P(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)\n",
    "$$\n",
    ":::\n",
    "\n",
    "Note that the sets $A$ and $A^c$ form a partition of the sample space because $A \\cap A^c = \\emptyset$ and $A \\cup A^c = \\Omega$.\n",
    "\n",
    ":::{#thm-total-probability}\n",
    "## Total Probability Theorem\n",
    "\n",
    "Let $A_1, A_2, \\ldots, A_n$ be a partition of the sample space $\\Omega$. For any event $B$ such that $P(B) > 0$, we have\n",
    "\n",
    "$$\n",
    "P(B) = \\sum_{i=1}^n P(B|A_i) \\cdot P(A_i)\n",
    "$$\n",
    ":::\n",
    ":::{.proof}\n",
    "\n",
    "The intersection of $B$ with the sample space is $B \\cap \\Omega = B$ as $B$ is a subset of $\\Omega$ (as any event).\n",
    "Note that the sets $B \\cap A_i$ are disjoint because the $A_i$ are disjoint. Therefore, we have\n",
    "\n",
    "$$\n",
    "B = B \\cap \\Omega\n",
    "$$\n",
    "\n",
    "Because the $A_i$ are a partition of the sample space, their union is the sample space:\n",
    "\n",
    "$$\n",
    "\\Omega = \\bigcup_{i=1}^n A_i\n",
    "$$\n",
    "\n",
    "In this way we can represent $B$ as the union of the intersections of $B$ with the $A_i$:\n",
    "\n",
    "$$\n",
    "B = B \\cap \\Omega = B \\cap \\left( \\bigcup_{i=1}^n A_i \\right) = \\bigcup_{i=1}^n (B \\cap A_i)\n",
    "$$\n",
    "\n",
    "The last step follows from the distributive property of the intersection over the union. Now we can use the additivity of the probability measure to write\n",
    "\n",
    "$$\n",
    "P(B) = P\\left( \\bigcup_{i=1}^n (B \\cap A_i) \\right) = \\sum_{i=1}^n P(B \\cap A_i) = \\sum_{i=1}^n P(B|A_i) \\cdot P(A_i)\n",
    "$$\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-narco-tests}\n",
    "## Roadside Drug Tests\n",
    "\n",
    "Imagine a city called Virtue City with 10,000 drivers in any day. Out of all drivers 100 of them use drugs while driving.\n",
    "\n",
    "The police department introduces a new test for narcotics that has a 95 percent probability of correctly identifying a person who has used narcotics (true positive). There is a one percent probability of falsely identifying a person who has not used narcotics as having used them (false positive).\n",
    "\n",
    "The police stop a driver, his test comes back positive, and the driver's car is confiscated. The police claim that there is a 95 percent probability that the driver is a drug user. The driver objects and says that the probability of him being a drug user is much lower. Who is right?\n",
    "\n",
    "Approach this problem in two ways:\n",
    "\n",
    "- Counting\n",
    "    - How many people will have a positive test result if all 100,000 citizens are tested?\n",
    "    - What proportion of the positive tests belong to actual drug users?\n",
    "- Apply Bayes' Theorem\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-spam-filter}\n",
    "## Spam Filtering\n",
    "\n",
    "If you have ever used email, then you have probably encountered spam emails. Spam emails are unsolicited messages that are commonly sent in bulk to many recipients. Most email service providers try to protect their users from spam by filtering out these messages. One way to do this is to train an algorithm to recognize spam emails based on the content of the email.\n",
    "\n",
    "Suppose that the filter receives an email containing the word \"Inheritance\". Suppose that 5 percent of spam emails contain this word, while it is only present in 0.1 percent of non-spam emails. Suppose that 80 percent of the emails that the filter receives are spam.\n",
    "\n",
    "What is the probability that an email containing the word \"Inheritance\" is spam?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-bayes-decomposition}\n",
    "## Decomposition of the Total Probability\n",
    "\n",
    "Prove the following statements:\n",
    "\n",
    "1. $P(B) = 1 \\implies P(A|B) = P(A)$ for any event $A$.\n",
    "2. $A \\subset B \\implies P(B|A) = 1$ and $P(A|B) = P(A)/P(B)$\n",
    "3. If $A \\cap B = \\emptyset$\n",
    "\n",
    "$$\n",
    "P(A | A \\cup B) = \\frac{P(A)}{P(A) + P(B)}\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{#thm-chain-rule}\n",
    "## Chain Rule\n",
    "\n",
    "Let $A$, $B$, and $C$ be events such that $P(A) > 0$, $P(B|A) > 0$, and $P(C|A \\cap B) > 0$. Then\n",
    "\n",
    "$$\n",
    "P(A \\cap B \\cap C) = P(A|B\\cap C) P(B|C) P(C)\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "## Proof (click to expand)\n",
    "\n",
    "The proof requires nothing more than the definition of conditional probability. Let $D = B \\cap C$. Then we can write the\n",
    "intersection of the three events as\n",
    "\n",
    "$$\n",
    "P(A \\cap B \\cap C) = P(A \\cap D) = P(A|D) P(D) = P(A|B \\cap C) P(B \\cap C)\n",
    "$$\n",
    "\n",
    "Apply the definition of conditional probability to the last term:\n",
    "\n",
    "$$\n",
    "P(A|B \\cap C) P(B \\cap C) = P(A|B \\cap C) P(B|C) P(C)\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Application of the Chain Rule\n",
    "\n",
    "Recent advances in natural language processing have made possible applications such as chatbots that can interact with users in a natural way. Earlier probabilistic language models were based on the probability of a sentence.\n",
    "\n",
    "Consider a simple sentence \n",
    "\n",
    "\"I am happy for you\". \n",
    "\n",
    "One way to estimate the probability of this sentence is to count how many times it appears in a large corpus of text. However, this approach is very limited because long sequences of words are unlikely to appear in any corpus more than once and may not appear at all.\n",
    "\n",
    "One way to work around this limitation is to express the probability of a sentence (joint occurrence of all the words in the sentence) as the product of the conditional probabilities of each word given the previous words using the chain rule.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_5, w_4, w_3, w_2, w_1) & = \\cdot P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4, w_3, w_2, w_1) \\\\\n",
    "& = P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3, w_2, w_1) \\\\\n",
    "& = P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2, w_1) \\\\\n",
    "& = P(w_5 | w_4, w_3, w_2, w_1) \\cdot P(w_4 | w_3, w_2, w_1) \\cdot P(w_3 | w_2, w_1) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can now simplify this expression by _assuming_ that the conditional probability of a word only depends on the word before it (Markov assumption).\n",
    "\n",
    "$$\n",
    "P(w_5, w_4, w_3, w_2, w_1) = P(w_5 | w_4) \\cdot P(w_4 | w_3) \\cdot P(w_3 | w_2) \\cdot P(w_2 | w_1) \\cdot P(w_1)\n",
    "$$\n",
    "\n",
    "It is now much simpler to estimate the conditional probabilities of each word given the previous word. We simply need to count how many times each word appears after the previous word in a large corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: click in /home/amarov/miniconda3/envs/abs/lib/python3.12/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /home/amarov/miniconda3/envs/abs/lib/python3.12/site-packages (from nltk) (1.4.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n",
      "\u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/amarov/miniconda3/envs/abs/lib/python3.12/site-packages (from nltk) (4.66.5)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m1.3/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\r\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/796.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.1/796.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m788.5/796.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: regex, nltk\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed nltk-3.9.1 regex-2024.11.6\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /home/amarov/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/amarov/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Alice in Wonderland from the gutenber corpus\n",
    "alice = gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequence 'Alice said hello' appears 0 times in the text.\n"
     ]
    }
   ],
   "source": [
    "# Use FreqDist to find the frequency of the specific sequence\n",
    "sequence = \"Alice said hello\"\n",
    "\n",
    "# Create n-grams from the tokens\n",
    "sequence_tokens = nltk.word_tokenize(sequence)\n",
    "tokens = nltk.word_tokenize(alice)\n",
    "ngrams_list = list(ngrams(tokens, len(sequence_tokens)))\n",
    "fdist = FreqDist(ngrams_list)\n",
    "\n",
    "sequence_frequency = fdist[tuple(sequence_tokens)]\n",
    "print(f\"The sequence '{sequence}' appears {sequence_frequency} times in the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', \"'\", 'said'), 215),\n",
       " ((\"'\", 'said', 'the'), 203),\n",
       " ((\"'\", 'said', 'Alice'), 115),\n",
       " (('.', \"'\", 'I'), 69),\n",
       " (('!', \"'\", 'said'), 65),\n",
       " ((',', \"'\", 'the'), 59),\n",
       " (('*', '*', '*'), 54),\n",
       " (('the', 'Mock', 'Turtle'), 49),\n",
       " ((\"'\", \"'\", 'I'), 47),\n",
       " ((',', 'and', 'the'), 42)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-quality-control}\n",
    "## Quality Control\n",
    "\n",
    "A factory produces light bulbs. To ensure the quality of the bulbs shipped to customers, the factory has a quality control system that checks a sample of the bulbs before they are shipped. Consider a shipment of 100 bulbs. You take a sample (without replacement) of 5 bulbs and stop the shipment if you find that at least one of the sampled bulbs are defective. Suppose that a shipment contains 20 defective bulbs. What is the probability that the shipment will be stopped?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shipment of light bulbs\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Some of the selected samples\n",
      "[0 1 0 0 0]\n",
      "\n",
      "\n",
      "The proportion of shipments rejected is 0.7\n"
     ]
    }
   ],
   "source": [
    "# Simulate a sample from a shipment of 100 light bulbs\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 5\n",
    "repetitions = 10\n",
    "\n",
    "# 0 represents a good light bulb, 1 represents a defective light bulb\n",
    "shipment = np.array([0] * 80 + [1] * 20)\n",
    "\n",
    "print(\"The shipment of light bulbs\")\n",
    "print(shipment)\n",
    "print(\"\\n\")\n",
    "\n",
    "rejections = 0\n",
    "\n",
    "print(\"Some of the selected samples\")\n",
    "\n",
    "for i in range(repetitions):\n",
    "    sample = np.random.choice(shipment, size = sample_size, replace = False)\n",
    "    \n",
    "    # Print the sample once every 1000 repetitions\n",
    "    if i % 1000 == 0:\n",
    "        print(sample)\n",
    "        \n",
    "    if np.sum(sample) > 0:\n",
    "        rejections += 1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The proportion of shipments rejected is {rejections / repetitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note collapse=\"true\"}\n",
    "## Solution (click to expand)\n",
    "\n",
    "We need to compute the probability that at least one of the sampled bulbs is defective, given that there are 20 defective bulbs in the shipment of 100 bulbs. We can define the event $A$ as the event that at least one of the sampled bulbs is defective.\n",
    "\n",
    "$$\n",
    "A = \\text{at least one defective bulb in the sample}\n",
    "$$\n",
    "\n",
    "Let $A_1, A_2, \\ldots, A_{5}$ be the events that the $i$-th bulb in the sample is defective. Then we have\n",
    "\n",
    "$$\n",
    "A = A_1 \\cup A_2 \\cup \\ldots \\cup A_{5}\n",
    "$$\n",
    "\n",
    "It is easy to compute the probability of the complement of $A$ using de Morgan's law\n",
    "\n",
    "$$\n",
    "P(A) = 1 - P(A^c) = 1 - P(A_1^c \\cap A_2^c \\cap \\ldots \\cap A_{5}^c)\n",
    "$$\n",
    "\n",
    "The chain rule gives us a way to decompose the probability of the intersections of the complements\n",
    "\n",
    "$$\n",
    "P(A_1^c \\cap A_2^c \\cap \\ldots \\cap A_{5}^c) = P(A_1^c) \\cdot P(A_2^c | A_1^c) \\cdot P(A_3^c | A_1^c \\cap A_2^c) \\cdot \\ldots \\cdot P(A_{5}^c | A_1^c \\cap A_2^c \\cap \\ldots \\cap A_{4}^c)\n",
    "$$\n",
    "\n",
    "The probability of the first bulb not being defective is\n",
    "\n",
    "$$\n",
    "P(A_1^c) = \\frac{80}{100}\n",
    "$$\n",
    "\n",
    "The probability of the second bulb not being defective given that the first bulb is not defective is\n",
    "\n",
    "$$\n",
    "P(A_2^c | A_1^c) = \\frac{79}{99}\n",
    "$$\n",
    "\n",
    "Continuing in this way, we find that\n",
    "\n",
    "$$\n",
    "P(A) = 1 - \\frac{80}{100} \\cdot \\frac{79}{99} \\cdot \\frac{78}{98} \\cdot \\frac{77}{97} \\cdot \\frac{76}{96}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability of a shipment being rejected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-simulation-dogs}\n",
    "## A Bifurcation in the Road\n",
    "\n",
    "A tourists stands at a bifurcation in the road and does not know which is the correct way to go. He asks two passersby. Each passerby points him in the correct direction with probability 0.7 independently of each other.\n",
    "We want to propose a strategy for the tourist to decide which way to go.\n",
    "\n",
    "- Strategy 1: If the two passersby point in the same direction, the tourist follows that direction. If they point in different directions, the tourist chooses randomly.\n",
    "- Strategy 2: The tourist decides to follow the direction of the first passerby only.\n",
    "- Strategy 3: The tourist decides at random.\n",
    "\n",
    "- Which strategy would you recommend? Decide this by calculating the probability of the tourist going in the correct direction for each strategy.\n",
    "- Simulate the three strategies and estimate the probability of the tourist going in the correct direction. For the sake of the simulation, assume that the correct direction is the first one.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passerby1</th>\n",
       "      <th>passerby2</th>\n",
       "      <th>random_choice</th>\n",
       "      <th>strategy1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passerby1  passerby2  random_choice  strategy1\n",
       "0          1          2              1          1\n",
       "1          1          1              2          1\n",
       "2          2          1              1          1\n",
       "3          1          1              2          1\n",
       "4          2          2              1          2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup the simulation\n",
    "repetitions = 10\n",
    "\n",
    "# Simulate the third strategy\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"passerby1\": np.random.choice([1, 2], size = repetitions, p = [0.7, 0.3]),\n",
    "    \"passerby2\": np.random.choice([1, 2], size = repetitions, p = [0.7, 0.3]),\n",
    "    \"random_choice\": np.random.choice([1, 2], size = repetitions, p = [0.5, 0.5])\n",
    "})\n",
    "\n",
    "# Checks if the two passerby agree on the direction. If yes, then take the direction of the first passerby \n",
    "# (both are equal). If not, then take the random direction\n",
    "\n",
    "df[\"strategy1\"] = np.where(\n",
    "    df[\"passerby1\"] == df[\"passerby2\"],\n",
    "    df[\"passerby1\"],\n",
    "    df[\"random_choice\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 1 success rate:  0.8\n",
      "Strategy 2 success rate:  0.7\n",
      "Strategy 3 success rate:  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Strategy 1 success rate: \", (df[\"strategy1\"] == 1).sum() / repetitions)\n",
    "print(\"Strategy 2 success rate: \", (df[\"passerby1\"] == 1).sum() / repetitions)\n",
    "print(\"Strategy 3 success rate: \", (df[\"random_choice\"] == 1).sum() / repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note collapse=\"true\"}\n",
    "## Solution (click to expand)\n",
    "\n",
    "Let us consider the scenarios leading to the correct choice, assuming the passersby are independent and point in the right direction with probability $p$ and in the wrong direction with probability $1-p$.\n",
    "\n",
    "- The passersby agree and are correct with probability $p^2$.\n",
    "- The first passerby is right and the the tourist follows his or her advice $p(1 - p)/2$.\n",
    "- The second passerby is right the tourist follows his or her advice:  $(1 - p)p /2$.\n",
    "\n",
    "These events are mutually exclusive and exhaustive, so the probability of the tourist going in the correct direction under Strategy 1 is\n",
    "\n",
    "$$\n",
    "p^2 + \\frac{p(1 - p)}{2} + \\frac{(1 - p)p}{2} = p^2 + p(1 - p) = p\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-monty-hall}\n",
    "## The Monty Hall Game\n",
    "\n",
    "![The Monty Hall Game](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/300px-Monty_open_door.svg.png)\n",
    "\n",
    "\n",
    "Imagine a game with three doors. Behind one of the doors is a car and behind the other two are goats. The game is played in two stages.\n",
    "\n",
    "- First you choose a door.\n",
    "- The host, _who knows what is behind each door_, opens one of the other two doors to reveal a goat.\n",
    "- You are then given the opportunity to switch to the other unopened door or stay with your original choice.\n",
    "\n",
    "Which strategy has a higher probability of winning the car? Should you switch or should you stay with your original choice? Compute the probabilities of winning the car under each strategy.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_choice</th>\n",
       "      <th>opened_door</th>\n",
       "      <th>switch_choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_choice  opened_door  switch_choice\n",
       "0             1            3              2\n",
       "1             2            3              1\n",
       "2             2            3              1\n",
       "3             3            2              1\n",
       "4             1            2              3\n",
       "5             2            3              1\n",
       "6             3            2              1\n",
       "7             3            2              1\n",
       "8             1            2              3\n",
       "9             2            3              1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulation of the Monty Hall problem\n",
    "\n",
    "# Setup the simulation\n",
    "repetitions = 10\n",
    "\n",
    "# Simulate the choices of the player\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"first_choice\": np.random.choice([1, 2, 3], size = repetitions),    \n",
    "})\n",
    "\n",
    "df[\"opened_door\"] = np.where(\n",
    "    # If the player chose door 1 (the door with the car)\n",
    "    (df[\"first_choice\"] == 1),\n",
    "    # , then the opened door is randomly chosen from doors 2 and 3\n",
    "    np.random.choice([2, 3], size = repetitions),\n",
    "    # Otherwise\n",
    "    np.where(\n",
    "        # If the player chose door 2\n",
    "        (df[\"first_choice\"] == 2),\n",
    "        # , then the opened door is 3 (the host does not open the door with the car)\n",
    "        3,\n",
    "        # Otherwise the opened door is\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"switch_choice\"] = np.where(\n",
    "    # If the opened door was 2 and the player chose 1\n",
    "    (df[\"opened_door\"] == 2) & (df[\"first_choice\"] == 1),\n",
    "    # , then the switched choice is 3\n",
    "    3,\n",
    "    # Otherwise\n",
    "    np.where(\n",
    "        # If the opened door was 3 and the player chose 1\n",
    "        (df[\"opened_door\"] == 3) & (df[\"first_choice\"] == 1),\n",
    "        # , then the switched choice is 2\n",
    "        2,\n",
    "        # Otherwise the switched choice is 1\n",
    "        1\n",
    "    )\n",
    ")\n",
    "\n",
    "df.head(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No switching wins 0.3\n",
      "Switching wins 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"No switching wins\", (df[\"first_choice\"] == 1).mean())\n",
    "print(\"Switching wins\", (df[\"switch_choice\"] == 1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "## Solution (click to expand)\n",
    "\n",
    "Under the first strategy of no switching, the probability of winning the car is $1/3$ because the car is equally likely to be behind any of the three doors.\n",
    "\n",
    "Under the second strategy of switching, the probability of winning the car is $2/3$. To see this, consider the following:\n",
    "\n",
    "- The probability of the car being behind the door you chose is $1/3$.\n",
    "- The probability of the car being behind one of the other two doors is $2/3$.\n",
    "- The host will always open a door with a goat behind it. Therefore, the car is behind one of the other two doors, so the probability of winning the car by switching is $2/3$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{#exr-prisoners}\n",
    "## The Executive's Dilemma\n",
    "\n",
    "Consider the following problem: A struggling company need to reduce its executive workforce from three executives to only one. The company has three executives: Alice, Bob, and Charlie. The company president decides to leave the decision of whom to keep to chance.\n",
    "\n",
    "The president does not want to tell the executives who is remaining until the decision is announced. Alice insists, however, and asks the president to tell her at least the name of one person who is sacked. The president reminds Alice that he will reveal the name of the person remaining and also tells her that Bob is sacked.\n",
    "\n",
    "Alice is happy because she thinks that the probability of her remaining given this information is $1/2$ instead of $1/3$ (without the information). Is Alice's reasoning correct?\n",
    "\n",
    "The example here is a reformulation of the famous [Prisoner's Dilemma](https://en.m.wikipedia.org/wiki/Three_prisoners_problem).\n",
    ":::\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "## Solution (click to expand)\n",
    "\n",
    "The probability of Alice remaining is $1/3$ regardless of the information given by the president. The reason for this lies _in the way_ the president chooses what to reveal. The president will always reveal the name of one person who is sacked, so the information given to Alice is not informative.\n",
    "\n",
    "As the president chooses the person to remain at random, the probability of Alice remaining is $1/3$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& P(\\text{Alice remains}) = \\frac{1}{3} \\\\\n",
    "& P(\\text{Bob remains}) = \\frac{1}{3} \\\\\n",
    "& P(\\text{Charlie remains}) = \\frac{1}{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Consider all (mutually exclusive) scenarios:\n",
    "\n",
    "- Alice remains and the president reveals Bob's name (probability $1/3 \\cdot 1/2 = 1/6$)\n",
    "- Alice remains and the president reveals Charlie's name (probability $1/3 \\cdot 1/2 = 1/6$)\n",
    "- Alice is sacked (Bob remains) and the president reveals Charlie's name (probability $1/3 \\cdot 1 = 1/3$)\n",
    "- Alice is sacked (Charlie remains) and the president reveals Bob's name (probability $1/3 \\cdot 1 = 1/3$)\n",
    "\n",
    "We are looking for the _conditional_ probability of Alice remaining given that the president reveals Bob's name. \n",
    "\n",
    "$$\n",
    "P(\\text{Alice remains} | \\text{Bob revealed}) = \\frac{P(\\text{Alice remains} \\cap \\text{Bob revealed})}{P(\\text{Bob revealed})} = \\frac{1/6}{1/2} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Bob revealed}) = P(\\text{Alice remains} \\cap \\text{Bob revealed}) + P(\\text{Alice is sacked} \\cap \\text{Bob revealed}) = 1/6 + 1/3 = 1/2\n",
    "$$\n",
    "\n",
    "Note that the events \"Alice remains\" and \"Alice is sacked\" are a partition (mutually exclusive and exhaustive) of the sample space.\n",
    "\n",
    "\n",
    "It is crucial to understand that the mechanism by which the president reveals the name of one person who is sacked is important. If the question the president answered was \"What will happen to Bob?\" and the answer was \"Bob is sacked\" then Alice should calculate the conditional probability of her remaining given that Bob is sacked. In this case the probability would be $1/2$.\n",
    "\n",
    "$$\n",
    "P(\\text{Alice remains} | \\text{Bob is sacked}) = \\frac{P(\\text{Alice remains} \\cap \\text{Bob is sacked})}{P(\\text{Bob is sacked})} = \\frac{1/6}{1/3} = \\frac{1}{2}\n",
    "$$\n",
    " \n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
