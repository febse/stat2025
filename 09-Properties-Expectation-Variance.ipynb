{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the Expectation and Variance\n",
    "\n",
    "Now that we are familiar with the concepts of a joint distribution, marginal distributions, variance and covariance, we can discuss some of their properties.\n",
    "\n",
    "### Expectation\n",
    "\n",
    "1. The expectation of a constant is the constant itself:\n",
    "\n",
    "$$\n",
    "E(c) = c\n",
    "$$\n",
    "\n",
    "2. The expectation of a sum of random variables is the sum of the expectations:\n",
    "\n",
    "$$\n",
    "E(X + Y) = E(X) + E(Y)\n",
    "$$\n",
    "\n",
    "To prove this, we can use the definition of the expectation. Let $f_{XY}(x, y)$ be the joint probability mass function of $X$ and $Y$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(X + Y) & = \\sum_x \\sum_y (x + y) f_{XY}(x, y) \\\\\n",
    "         & = \\sum_x \\sum_y x f_{XY}(x, y) + \\sum_x \\sum_y y f_{XY}(x, y) \\\\\n",
    "         & = \\sum_x x \\sum_y f_{XY}(x, y) + \\sum_y y \\sum_x f_{XY}(x, y) \\\\\n",
    "         & = \\sum_x x f_X(x) + \\sum_y y f_Y(y) \\\\\n",
    "         & = E(X) + E(Y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the derivation above we use the fact that the marginal distribution of $X$ is\n",
    "\n",
    "$$\n",
    "f_X(x) = \\sum_y f_{XY}(x, y)\n",
    "$$\n",
    "\n",
    "and the marginal distribution of $Y$ is\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\sum_x f_{XY}(x, y)\n",
    "$$\n",
    "\n",
    "3. The expectation of a constant times a random variable is the constant times the expectation of the random variable:\n",
    "\n",
    "$$\n",
    "E(cX) = cE(X)\n",
    "$$\n",
    "\n",
    "To prove this, we can use the definition of the expectation. Let $f_{X}(x)$ be the probability mass function of $X$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(cX) & = \\sum_x c x f_{X}(x) \\\\\n",
    "       & = c \\sum_x x f_{X}(x) \\\\\n",
    "       & = c E(X)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here we use the fact that the constant does not depend on the summation index $x$ and can therefore be taken out of the summation.\n",
    "\n",
    "Now we can combine these two properties to show that the expectation of a linear combination of random variables is the linear combination of the expectations:\n",
    "\n",
    "$$\n",
    "E(aX + bY) = aE(X) + bE(Y)\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "\n",
    "1. The variance of a constant is zero:\n",
    "\n",
    "$$\n",
    "Var(c) = 0\n",
    "$$\n",
    "\n",
    "To see this, we can use the definition of the variance. Let $f_{X}(x)$ be the probability mass function of $X$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(c) & = E((c - E(c))^2) \\\\\n",
    "       & = E((c - c)^2) \\\\\n",
    "       & = E(0) \\\\\n",
    "       & = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here use use the first property of the expectation (the expectation of a constant is the constant itself).\n",
    "\n",
    "2. The variance of a sum of random variables is the sum of the variances plus twice the covariance:\n",
    "\n",
    "$$\n",
    "Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\n",
    "$$\n",
    "\n",
    "To prove this, we can use the definition of the variance. Let $f_{XY}(x, y)$ be the joint probability mass function of $X$ and $Y$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(X + Y) & = E((X + Y - E(X + Y))^2) \\\\\n",
    "           & = E((X + Y - E(X) - E(Y))^2) \\\\\n",
    "           & = E((X - E(X))^2 + (Y - E(Y))^2 + 2(X - E(X))(Y - E(Y))) \\\\\n",
    "           & = E((X - E(X))^2) + E((Y - E(Y))^2) + 2E((X - E(X))(Y - E(Y))) \\\\\n",
    "           & = Var(X) + Var(Y) + 2Cov(X, Y)\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here we use the fact that the expectation is a linear operator and that the expectation of a sum is the sum of the expectations. The rest follows just from rearranging terms and using the definition of the covariance.\n",
    "\n",
    "3. The variance of a sum of random variables is the sum of the variances if the random variables are uncorrelated (i.e., the covariance is zero):\n",
    "\n",
    "$$\n",
    "Var(X + Y) = Var(X) + Var(Y)\n",
    "$$\n",
    "\n",
    "This follows directly from the second property of the variance. If the random variables are uncorrelated, the covariance is zero, and the variance of the sum is the sum of the variances.\n",
    "\n",
    "4. The variance of a constant times a random variable is the constant squared times the variance of the random variable:\n",
    "\n",
    "$$\n",
    "Var(cX) = c^2 Var(X)\n",
    "$$\n",
    "\n",
    "This follows directly from the definition of the variance and the expectation of a constant times a random variable.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(cX) & = E((cX - E(cX))^2) \\\\\n",
    "        & = E((cX - cE(X))^2) \\\\\n",
    "        & = E(c^2(X - E(X))^2) \\\\\n",
    "        & = c^2 E((X - E(X))^2) \\\\\n",
    "        & = c^2 Var(X)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The proof only uses the property of the expectation (the expectation of a constant times a random variable is the constant times the expectation of the random variable) and the definition of the variance.\n",
    "\n",
    "5. The variance of a linear combination of random variables is the linear combination of the variances plus twice the covariance:\n",
    "\n",
    "$$\n",
    "Var(aX + bY) = a^2 Var(X) + b^2 Var(Y) + 2ab Cov(X, Y)\n",
    "$$\n",
    "\n",
    "This follows directly from the definition of the variance and the expectation of a linear combination of random variables.\n",
    "\n",
    "6. The variance can be rewritten as:\n",
    "\n",
    "$$\n",
    "Var(X) = E(X^2) - E(X)^2\n",
    "$$\n",
    "\n",
    "This follows directly from the definition of the variance.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(X) & = E((X - E(X))^2) \\\\\n",
    "       & = E(X^2 - 2XE(X) + E(X)^2) \\\\\n",
    "       & = E(X^2) - 2E(X)E(X) + E(X)^2 \\\\\n",
    "       & = E(X^2) - E(X)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "7. The covariance can be rewritten as:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = E(XY) - E(X)E(Y)\n",
    "$$\n",
    "\n",
    "This follows directly from the definition of the covariance.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov(X, Y) & = E((X - E(X))(Y - E(Y))) \\\\\n",
    "          & = E(XY - XE(Y) - YE(X) + E(X)E(Y)) \\\\\n",
    "          & = E(XY) - E(X)E(Y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Independence and Covariance\n",
    "\n",
    "If two random variables are independent, their covariance is zero. This follows directly from the definition of independence and the definition of the covariance. By definition, independence means that the joint probability mass function can be written as the product of the marginal probability mass functions:\n",
    "\n",
    "\n",
    "$$\n",
    "P(X = x, Y = y) = P(X = x)P(Y = y)\n",
    "$$\n",
    "\n",
    "This means that the expected value of the product of the random variables is the product of the expected values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(XY) & = \\sum_x \\sum_y xy P(X = x, Y = y) \\\\\n",
    "       & = \\sum_x \\sum_y xy P(X = x)P(Y = y) \\\\\n",
    "       & = \\sum_x x P(X = x) \\sum_y y P(Y = y) \\\\\n",
    "       & = E(X)E(Y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The definition of the covariance is:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = E(XY) - E(X)E(Y)\n",
    "$$\n",
    "\n",
    "Substitute the expected value of the product of the random variables:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = E(X)E(Y) - E(X)E(Y) = 0\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
